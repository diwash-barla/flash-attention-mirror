#blocked = #ttg.blocked<{sizePerThread = [1, 8], threadsPerWarp = [4, 16], warpsPerCTA = [4, 1], order = [1, 0]}>
#blocked1 = #ttg.blocked<{sizePerThread = [8, 1], threadsPerWarp = [16, 4], warpsPerCTA = [1, 4], order = [0, 1]}>
#linear = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 16], [0, 32], [0, 64], [128, 0]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[32, 0], [64, 0]], block = []}>
#linear1 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [32, 0], [64, 0]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [16, 0], [0, 8]], warp = [[0, 0], [0, 0]], block = []}>
#linear2 = #ttg.linear<{register = [[0, 1], [0, 2], [0, 4], [0, 32], [0, 64]], lane = [[1, 0], [2, 0], [4, 0], [8, 0], [0, 8], [0, 16]], warp = [[0, 0], [0, 0]], block = []}>
#linear3 = #ttg.linear<{register = [[1, 0], [2, 0], [0, 16], [0, 128]], lane = [[0, 1], [0, 2], [0, 4], [0, 8], [4, 0], [8, 0]], warp = [[0, 32], [0, 64]], block = []}>
#loc = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1699:0)
#mma = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 1], instrShape = [32, 32, 16], isTransposed = true}>
#mma1 = #ttg.amd_mfma<{version = 4, warpsPerCTA = [1, 4], instrShape = [16, 16, 32], isTransposed = true}>
#mma2 = #ttg.amd_mfma<{version = 4, warpsPerCTA = [4, 1], instrShape = [16, 16, 32], isTransposed = true, tilesPerWarp = [2, 1]}>
#shared = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 16, order = [1, 0]}>
#shared1 = #ttg.swizzled_shared<{vec = 8, perPhase = 1, maxPhase = 16, order = [0, 1]}>
#shared2 = #ttg.swizzled_shared<{vec = 8, perPhase = 8, maxPhase = 2, order = [0, 1]}>
#smem = #ttg.shared_memory
#loc78 = loc("Q"(#loc))
#loc79 = loc("K"(#loc))
#loc80 = loc("V"(#loc))
#loc81 = loc("sm_scale"(#loc))
#loc82 = loc("DO"(#loc))
#loc83 = loc("DK"(#loc))
#loc84 = loc("DV"(#loc))
#loc85 = loc("DQ"(#loc))
#loc86 = loc("M"(#loc))
#loc87 = loc("Delta"(#loc))
#loc88 = loc("stride_qb"(#loc))
#loc89 = loc("stride_qh"(#loc))
#loc90 = loc("stride_qm"(#loc))
#loc91 = loc("stride_kb"(#loc))
#loc92 = loc("stride_kh"(#loc))
#loc93 = loc("stride_kn"(#loc))
#loc94 = loc("stride_vb"(#loc))
#loc95 = loc("stride_vh"(#loc))
#loc96 = loc("stride_vn"(#loc))
#loc97 = loc("stride_dkb"(#loc))
#loc98 = loc("stride_dkh"(#loc))
#loc99 = loc("stride_dkn"(#loc))
#loc100 = loc("stride_dqb"(#loc))
#loc101 = loc("stride_dqh"(#loc))
#loc102 = loc("stride_dqm"(#loc))
#loc103 = loc("stride_deltab"(#loc))
#loc104 = loc("stride_deltah"(#loc))
#loc105 = loc("stride_dob"(#loc))
#loc106 = loc("stride_doh"(#loc))
#loc107 = loc("stride_dom"(#loc))
#loc108 = loc("stride_dropoutb"(#loc))
#loc109 = loc("stride_dropouth"(#loc))
#loc110 = loc("stride_dropoutm"(#loc))
#loc111 = loc("stride_dropoutn"(#loc))
#loc112 = loc("max_seqlen_q"(#loc))
#loc113 = loc("max_seqlen_k"(#loc))
#loc114 = loc("dropout_p"(#loc))
#loc115 = loc("philox_seed"(#loc))
#loc116 = loc("philox_offset"(#loc))
#loc117 = loc("NUM_K_PIDS"(#loc))
module attributes {"ttg.num-ctas" = 1 : i32, "ttg.num-warps" = 4 : i32, ttg.target = "hip:gfx950", "ttg.threads-per-warp" = 64 : i32} {
  tt.func public @_bwd_kernel_fused_atomics_dkdvdq_noncausal(%Q: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Q"(#loc)), %K: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("K"(#loc)), %V: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("V"(#loc)), %sm_scale: f32 loc("sm_scale"(#loc)), %DO: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DO"(#loc)), %DK: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DK"(#loc)), %DV: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DV"(#loc)), %DQ: !tt.ptr<f16> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("DQ"(#loc)), %M: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("M"(#loc)), %Delta: !tt.ptr<f32> {tt.divisibility = 16 : i32, tt.pointer_range = 32 : i32} loc("Delta"(#loc)), %stride_qb: i32 {tt.divisibility = 16 : i32} loc("stride_qb"(#loc)), %stride_qh: i32 {tt.divisibility = 16 : i32} loc("stride_qh"(#loc)), %stride_qm: i32 {tt.divisibility = 16 : i32} loc("stride_qm"(#loc)), %stride_kb: i32 {tt.divisibility = 16 : i32} loc("stride_kb"(#loc)), %stride_kh: i32 {tt.divisibility = 16 : i32} loc("stride_kh"(#loc)), %stride_kn: i32 {tt.divisibility = 16 : i32} loc("stride_kn"(#loc)), %stride_vb: i32 {tt.divisibility = 16 : i32} loc("stride_vb"(#loc)), %stride_vh: i32 {tt.divisibility = 16 : i32} loc("stride_vh"(#loc)), %stride_vn: i32 {tt.divisibility = 16 : i32} loc("stride_vn"(#loc)), %stride_dkb: i32 {tt.divisibility = 16 : i32} loc("stride_dkb"(#loc)), %stride_dkh: i32 {tt.divisibility = 16 : i32} loc("stride_dkh"(#loc)), %stride_dkn: i32 {tt.divisibility = 16 : i32} loc("stride_dkn"(#loc)), %stride_dqb: i32 {tt.divisibility = 16 : i32} loc("stride_dqb"(#loc)), %stride_dqh: i32 {tt.divisibility = 16 : i32} loc("stride_dqh"(#loc)), %stride_dqm: i32 {tt.divisibility = 16 : i32} loc("stride_dqm"(#loc)), %stride_deltab: i32 {tt.divisibility = 16 : i32} loc("stride_deltab"(#loc)), %stride_deltah: i32 {tt.divisibility = 16 : i32} loc("stride_deltah"(#loc)), %stride_dob: i32 {tt.divisibility = 16 : i32} loc("stride_dob"(#loc)), %stride_doh: i32 {tt.divisibility = 16 : i32} loc("stride_doh"(#loc)), %stride_dom: i32 {tt.divisibility = 16 : i32} loc("stride_dom"(#loc)), %stride_dropoutb: i32 {tt.divisibility = 16 : i32} loc("stride_dropoutb"(#loc)), %stride_dropouth: i32 {tt.divisibility = 16 : i32} loc("stride_dropouth"(#loc)), %stride_dropoutm: i32 {tt.divisibility = 16 : i32} loc("stride_dropoutm"(#loc)), %stride_dropoutn: i32 {tt.divisibility = 16 : i32} loc("stride_dropoutn"(#loc)), %max_seqlen_q: i32 {tt.divisibility = 16 : i32} loc("max_seqlen_q"(#loc)), %max_seqlen_k: i32 {tt.divisibility = 16 : i32} loc("max_seqlen_k"(#loc)), %dropout_p: f32 loc("dropout_p"(#loc)), %philox_seed: i32 loc("philox_seed"(#loc)), %philox_offset: i32 loc("philox_offset"(#loc)), %NUM_K_PIDS: i32 {tt.divisibility = 16 : i32} loc("NUM_K_PIDS"(#loc))) attributes {noinline = false} {
    %true = arith.constant true loc(#loc1)
    %cst = arith.constant dense<0.000000e+00> : tensor<256x128xf32, #mma> loc(#loc1)
    %c15_i32 = arith.constant 15 : i32 loc(#loc1)
    %c1_i32 = arith.constant 1 : i32 loc(#loc1)
    %c16_i32 = arith.constant 16 : i32 loc(#loc1)
    %c0_i32 = arith.constant 0 : i32 loc(#loc1)
    %c256_i32 = arith.constant 256 : i32 loc(#loc1)
    %c17_i32 = arith.constant 17 : i32 loc(#loc1)
    %cst_0 = arith.constant dense<0.000000e+00> : tensor<16x128xf32, #mma1> loc(#loc1)
    %cst_1 = arith.constant dense<0.000000e+00> : tensor<256x16xf32, #mma2> loc(#loc1)
    %cst_2 = arith.constant dense<0.000000e+00> : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc1)
    %wid = tt.get_program_id x : i32 loc(#loc118)
    %hkid = arith.remsi %wid, %c16_i32 : i32 loc(#loc119)
    %pid = arith.divsi %wid, %c16_i32 : i32 loc(#loc120)
    %pid_3 = arith.remsi %pid, %NUM_K_PIDS : i32 loc(#loc121)
    %start_n = arith.muli %pid_3, %c256_i32 : i32 loc(#loc122)
    %offs_n = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc123)
    %offs_n_4 = tt.make_range {end = 256 : i32, start = 0 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc123)
    %offs_n_5 = tt.splat %start_n : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc124)
    %offs_n_6 = tt.splat %start_n : i32 -> tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc124)
    %offs_n_7 = arith.addi %offs_n_5, %offs_n : tensor<256xi32, #ttg.slice<{dim = 1, parent = #linear}>> loc(#loc124)
    %offs_n_8 = arith.addi %offs_n_6, %offs_n_4 : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc124)
    %mask_kv = tt.expand_dims %offs_n_7 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<256x1xi32, #linear> loc(#loc125)
    %mask_kv_9 = tt.expand_dims %offs_n_8 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xi32, #blocked> loc(#loc125)
    %mask_kv_10 = tt.splat %max_seqlen_k : i32 -> tensor<256x1xi32, #linear> loc(#loc126)
    %mask_kv_11 = tt.splat %max_seqlen_k : i32 -> tensor<256x1xi32, #blocked> loc(#loc126)
    %mask_kv_12 = arith.cmpi slt, %mask_kv, %mask_kv_10 : tensor<256x1xi32, #linear> loc(#loc126)
    %mask_kv_13 = arith.cmpi slt, %mask_kv_9, %mask_kv_11 : tensor<256x1xi32, #blocked> loc(#loc126)
    %adj_k = arith.muli %hkid, %stride_kh : i32 loc(#loc127)
    %adj_k_14 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #linear}>> loc(#loc128)
    %adj_k_15 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> loc(#loc128)
    %adj_k_16 = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma1}>> loc(#loc128)
    %adj_v = arith.muli %hkid, %stride_vh : i32 loc(#loc129)
    %k = tt.expand_dims %offs_n_4 {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<256x1xi32, #blocked> loc(#loc130)
    %k_17 = arith.muli %start_n, %stride_kn : i32 loc(#loc130)
    %k_18 = tt.splat %stride_kn : i32 -> tensor<256x1xi32, #blocked> loc(#loc130)
    %k_19 = arith.muli %k, %k_18 : tensor<256x1xi32, #blocked> loc(#loc130)
    %k_20 = arith.addi %adj_k, %k_17 : i32 loc(#loc130)
    %k_21 = tt.broadcast %k_19 : tensor<256x1xi32, #blocked> -> tensor<256x128xi32, #blocked> loc(#loc130)
    %k_22 = tt.expand_dims %adj_k_15 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #blocked}>> -> tensor<1x128xi32, #blocked> loc(#loc130)
    %k_23 = tt.broadcast %k_22 : tensor<1x128xi32, #blocked> -> tensor<256x128xi32, #blocked> loc(#loc130)
    %k_24 = arith.addi %k_21, %k_23 : tensor<256x128xi32, #blocked> loc(#loc130)
    %k_25 = tt.splat %k_20 : i32 -> tensor<256x128xi32, #blocked> loc(#loc130)
    %k_26 = arith.addi %k_25, %k_24 : tensor<256x128xi32, #blocked> loc(#loc130)
    %k_27 = tt.broadcast %mask_kv_12 : tensor<256x1xi1, #linear> -> tensor<256x128xi1, #linear> loc(#loc131)
    %k_28 = tt.broadcast %mask_kv_13 : tensor<256x1xi1, #blocked> -> tensor<256x128xi1, #blocked> loc(#loc131)
    %k_29 = amdgpu.buffer_load %K[%k_26], %k_28 : tensor<256x128xf16, #blocked> loc(#loc131)
    %k_30 = ttg.local_alloc %k_29 : (tensor<256x128xf16, #blocked>) -> !ttg.memdesc<256x128xf16, #shared, #smem> loc(#loc131)
    %k_31 = ttg.local_load %k_30 : !ttg.memdesc<256x128xf16, #shared, #smem> -> tensor<256x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 8}>> loc(#loc131)
    %k_32 = ttg.local_load %k_30 : !ttg.memdesc<256x128xf16, #shared, #smem> -> tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> loc(#loc131)
    %v = arith.muli %start_n, %stride_vn : i32 loc(#loc132)
    %v_33 = tt.splat %stride_vn : i32 -> tensor<256x1xi32, #blocked> loc(#loc132)
    %v_34 = arith.muli %k, %v_33 : tensor<256x1xi32, #blocked> loc(#loc132)
    %v_35 = arith.addi %adj_v, %v : i32 loc(#loc132)
    %v_36 = tt.broadcast %v_34 : tensor<256x1xi32, #blocked> -> tensor<256x128xi32, #blocked> loc(#loc132)
    %v_37 = arith.addi %v_36, %k_23 : tensor<256x128xi32, #blocked> loc(#loc132)
    %v_38 = tt.splat %v_35 : i32 -> tensor<256x128xi32, #blocked> loc(#loc132)
    %v_39 = arith.addi %v_38, %v_37 : tensor<256x128xi32, #blocked> loc(#loc132)
    %v_40 = amdgpu.buffer_load %V[%v_39], %k_28 : tensor<256x128xf16, #blocked> loc(#loc133)
    %v_41 = ttg.local_alloc %v_40 : (tensor<256x128xf16, #blocked>) -> !ttg.memdesc<256x128xf16, #shared, #smem> loc(#loc133)
    %v_42 = ttg.local_load %v_41 : !ttg.memdesc<256x128xf16, #shared, #smem> -> tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> loc(#loc133)
    %0 = arith.addi %hkid, %c1_i32 : i32 loc(#loc18)
    %num_steps = arith.addi %max_seqlen_q, %c15_i32 : i32 loc(#loc185)
    %num_steps_43 = arith.divsi %num_steps, %c16_i32 : i32 loc(#loc186)
    %offs_m = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc187)
    %offs_m_44 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc187)
    %offs_m_45 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc187)
    %offs_m_46 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc187)
    %qT_ptrs_start = tt.make_range {end = 128 : i32, start = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> loc(#loc188)
    %start_idx = arith.muli %pid_3, %c17_i32 : i32 loc(#loc189)
    %start_idx_47 = arith.remsi %start_idx, %num_steps_43 : i32 loc(#loc190)
    %mask_m = tt.splat %max_seqlen_q : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc191)
    %mask_m_48 = tt.splat %max_seqlen_q : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc191)
    %mask_m_49 = tt.splat %max_seqlen_q : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
    %mask_m_50 = tt.splat %max_seqlen_q : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc191)
    %pT = tt.splat %sm_scale : f32 -> tensor<256x16xf32, #mma2> loc(#loc192)
    %1 = tt.splat %sm_scale : f32 -> tensor<16x128xf32, #mma1> loc(#loc141)
    %dv:2 = scf.for %dv_52 = %hkid to %0 step %c1_i32 iter_args(%arg41 = %cst, %arg42 = %cst) -> (tensor<256x128xf32, #mma>, tensor<256x128xf32, #mma>)  : i32 {
      %adj_q = arith.muli %dv_52, %stride_qh : i32 loc(#loc143)
      %adj_dq = arith.muli %dv_52, %stride_dqh : i32 loc(#loc144)
      %adj_do = arith.muli %dv_52, %stride_doh : i32 loc(#loc145)
      %adj_delta = arith.muli %dv_52, %stride_deltah : i32 loc(#loc146)
      llvm.intr.assume %true : i1 loc(#loc147)
      %m = tt.splat %adj_delta : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc194)
      %qT = ttg.local_alloc : () -> !ttg.memdesc<1x128x16xf16, #shared1, #smem, mutable> loc(#loc195)
      %do = ttg.local_alloc : () -> !ttg.memdesc<1x16x128xf16, #shared, #smem, mutable> loc(#loc196)
      %blk_idx = arith.remsi %start_idx_47, %num_steps_43 : i32 loc(#loc197)
      %curr_m = arith.muli %blk_idx, %c16_i32 : i32 loc(#loc198)
      %qT_ptrs = arith.muli %curr_m, %stride_qm : i32 loc(#loc199)
      %qT_ptrs_53 = tt.splat %qT_ptrs : i32 -> tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_54 = tt.expand_dims %offs_m {axis = 0 : i32} : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_55 = tt.splat %stride_qm : i32 -> tensor<1x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_56 = arith.muli %qT_ptrs_54, %qT_ptrs_55 : tensor<1x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_57 = tt.broadcast %qT_ptrs_56 : tensor<1x16xi32, #blocked1> -> tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_58 = tt.expand_dims %qT_ptrs_start {axis = 1 : i32} : tensor<128xi32, #ttg.slice<{dim = 1, parent = #blocked1}>> -> tensor<128x1xi32, #blocked1> loc(#loc200)
      %qT_ptrs_59 = tt.broadcast %qT_ptrs_58 : tensor<128x1xi32, #blocked1> -> tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_60 = arith.addi %qT_ptrs_57, %qT_ptrs_59 : tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_61 = arith.addi %qT_ptrs_60, %qT_ptrs_53 : tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_62 = tt.splat %adj_q : i32 -> tensor<128x16xi32, #blocked1> loc(#loc200)
      %qT_ptrs_63 = arith.addi %qT_ptrs_62, %qT_ptrs_61 : tensor<128x16xi32, #blocked1> loc(#loc200)
      %do_ptrs = arith.muli %curr_m, %stride_dom : i32 loc(#loc201)
      %do_ptrs_64 = tt.splat %do_ptrs : i32 -> tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_65 = tt.expand_dims %offs_m_45 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi32, #blocked> loc(#loc202)
      %do_ptrs_66 = tt.splat %stride_dom : i32 -> tensor<16x1xi32, #blocked> loc(#loc202)
      %do_ptrs_67 = arith.muli %do_ptrs_65, %do_ptrs_66 : tensor<16x1xi32, #blocked> loc(#loc202)
      %do_ptrs_68 = tt.broadcast %do_ptrs_67 : tensor<16x1xi32, #blocked> -> tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_69 = tt.broadcast %k_22 : tensor<1x128xi32, #blocked> -> tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_70 = arith.addi %do_ptrs_68, %do_ptrs_69 : tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_71 = arith.addi %do_ptrs_70, %do_ptrs_64 : tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_72 = tt.splat %adj_do : i32 -> tensor<16x128xi32, #blocked> loc(#loc202)
      %do_ptrs_73 = arith.addi %do_ptrs_72, %do_ptrs_71 : tensor<16x128xi32, #blocked> loc(#loc202)
      %offs_m_74 = tt.splat %curr_m : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc203)
      %offs_m_75 = tt.splat %curr_m : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc203)
      %offs_m_76 = tt.splat %curr_m : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
      %offs_m_77 = arith.addi %offs_m_74, %offs_m : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc203)
      %offs_m_78 = arith.addi %offs_m_75, %offs_m_44 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc203)
      %offs_m_79 = arith.addi %offs_m_76, %offs_m_45 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
      %mask_m_80 = arith.cmpi slt, %offs_m_77, %mask_m : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc191)
      %mask_m_81 = arith.cmpi slt, %offs_m_78, %mask_m_48 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc191)
      %mask_m_82 = arith.cmpi slt, %offs_m_79, %mask_m_49 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
      %mask_qT = tt.expand_dims %mask_m_80 {axis = 0 : i32} : tensor<16xi1, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x16xi1, #blocked1> loc(#loc204)
      %mask_do = tt.expand_dims %mask_m_82 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi1, #blocked> loc(#loc205)
      %qT_83 = tt.broadcast %mask_qT : tensor<1x16xi1, #blocked1> -> tensor<128x16xi1, #blocked1> loc(#loc195)
      %qT_84 = amdgpu.buffer_load %Q[%qT_ptrs_63], %qT_83 : tensor<128x16xf16, #blocked1> loc(#loc195)
      %m_85 = arith.addi %offs_m_44, %m : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc194)
      %m_86 = arith.addi %offs_m_75, %m_85 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc194)
      %m_87 = tt.splat %M : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc206)
      %m_88 = tt.addptr %m_87, %m_86 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>>, tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc206)
      %m_89 = tt.load %m_88, %mask_m_81, %cst_2 {amd.pipeliner_part = "prologue"} : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc206)
      %do_90 = tt.broadcast %mask_do : tensor<16x1xi1, #blocked> -> tensor<16x128xi1, #blocked> loc(#loc196)
      %do_91 = amdgpu.buffer_load %DO[%do_ptrs_73], %do_90 : tensor<16x128xf16, #blocked> loc(#loc196)
      %Di = tt.splat %Delta : !tt.ptr<f32> -> tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc207)
      %Di_92 = tt.addptr %Di, %m_86 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>>, tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc207)
      %Di_93 = tt.load %Di_92, %mask_m_81 {amd.pipeliner_part = "prologue"} : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc207)
      %qT_94 = ttg.memdesc_index %qT[%c0_i32] : !ttg.memdesc<1x128x16xf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> loc(#loc195)
      ttg.local_store %qT_84, %qT_94 : tensor<128x16xf16, #blocked1> -> !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> loc(#loc195)
      %do_95 = ttg.memdesc_index %do[%c0_i32] : !ttg.memdesc<1x16x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> loc(#loc196)
      ttg.local_store %do_91, %do_95 : tensor<16x128xf16, #blocked> -> !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> loc(#loc196)
      %curr_m_96 = arith.subi %num_steps_43, %c1_i32 : i32 loc(#loc230)
      %curr_m_97:8 = scf.for %curr_m_136 = %c0_i32 to %curr_m_96 step %c1_i32 iter_args(%arg44 = %arg41, %arg45 = %arg42, %curr_m_137 = %c0_i32, %curr_m_138 = %curr_m, %qT_139 = %qT_94, %m_140 = %m_89, %do_141 = %do_95, %Di_142 = %Di_93) -> (tensor<256x128xf32, #mma>, tensor<256x128xf32, #mma>, i32, i32, !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16>, tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>>, !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128>, tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>>)  : i32 {
        %curr_m_143 = arith.addi %curr_m_136, %c1_i32 : i32 loc(#loc230)
        %blk_idx_144 = arith.addi %start_idx_47, %curr_m_143 : i32 loc(#loc209)
        %blk_idx_145 = arith.remsi %blk_idx_144, %num_steps_43 : i32 loc(#loc197)
        %curr_m_146 = arith.muli %blk_idx_145, %c16_i32 : i32 loc(#loc198)
        %qT_ptrs_147 = arith.muli %curr_m_146, %stride_qm : i32 loc(#loc199)
        %qT_ptrs_148 = tt.splat %qT_ptrs_147 : i32 -> tensor<128x16xi32, #blocked1> loc(#loc200)
        %qT_ptrs_149 = arith.addi %qT_ptrs_60, %qT_ptrs_148 : tensor<128x16xi32, #blocked1> loc(#loc200)
        %qT_ptrs_150 = arith.addi %qT_ptrs_62, %qT_ptrs_149 : tensor<128x16xi32, #blocked1> loc(#loc200)
        %dq_ptrs_151 = arith.muli %curr_m_138, %stride_dqm : i32 loc(#loc210)
        %dq_ptrs_152 = tt.splat %dq_ptrs_151 : i32 -> tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_153 = tt.expand_dims %offs_m_46 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<16x1xi32, #mma1> loc(#loc211)
        %dq_ptrs_154 = tt.splat %stride_dqm : i32 -> tensor<16x1xi32, #mma1> loc(#loc211)
        %dq_ptrs_155 = arith.muli %dq_ptrs_153, %dq_ptrs_154 : tensor<16x1xi32, #mma1> loc(#loc211)
        %dq_ptrs_156 = tt.broadcast %dq_ptrs_155 : tensor<16x1xi32, #mma1> -> tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_157 = tt.expand_dims %adj_k_16 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_158 = tt.broadcast %dq_ptrs_157 : tensor<1x128xi32, #mma1> -> tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_159 = arith.addi %dq_ptrs_156, %dq_ptrs_158 : tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_160 = arith.addi %dq_ptrs_159, %dq_ptrs_152 : tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_161 = tt.splat %adj_dq : i32 -> tensor<16x128xi32, #mma1> loc(#loc211)
        %dq_ptrs_162 = arith.addi %dq_ptrs_161, %dq_ptrs_160 : tensor<16x128xi32, #mma1> loc(#loc211)
        %do_ptrs_163 = arith.muli %curr_m_146, %stride_dom : i32 loc(#loc201)
        %do_ptrs_164 = tt.splat %do_ptrs_163 : i32 -> tensor<16x128xi32, #blocked> loc(#loc202)
        %do_ptrs_165 = arith.addi %do_ptrs_70, %do_ptrs_164 : tensor<16x128xi32, #blocked> loc(#loc202)
        %do_ptrs_166 = arith.addi %do_ptrs_72, %do_ptrs_165 : tensor<16x128xi32, #blocked> loc(#loc202)
        %offs_m_167 = tt.splat %curr_m_146 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc203)
        %offs_m_168 = tt.splat %curr_m_146 : i32 -> tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc203)
        %offs_m_169 = tt.splat %curr_m_146 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
        %offs_m_170 = tt.splat %curr_m_138 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc203)
        %offs_m_171 = arith.addi %offs_m_167, %offs_m : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc203)
        %offs_m_172 = arith.addi %offs_m_168, %offs_m_44 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc203)
        %offs_m_173 = arith.addi %offs_m_169, %offs_m_45 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc203)
        %offs_m_174 = arith.addi %offs_m_170, %offs_m_46 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc203)
        %mask_m_175 = arith.cmpi slt, %offs_m_171, %mask_m : tensor<16xi32, #ttg.slice<{dim = 0, parent = #blocked1}>> loc(#loc191)
        %mask_m_176 = arith.cmpi slt, %offs_m_172, %mask_m_48 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc191)
        %mask_m_177 = arith.cmpi slt, %offs_m_173, %mask_m_49 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #blocked}>> loc(#loc191)
        %mask_m_178 = arith.cmpi slt, %offs_m_174, %mask_m_50 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc191)
        %mask_qT_179 = tt.expand_dims %mask_m_175 {axis = 0 : i32} : tensor<16xi1, #ttg.slice<{dim = 0, parent = #blocked1}>> -> tensor<1x16xi1, #blocked1> loc(#loc204)
        %mask_do_180 = tt.expand_dims %mask_m_177 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #blocked}>> -> tensor<16x1xi1, #blocked> loc(#loc205)
        %mask_do_181 = tt.expand_dims %mask_m_178 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<16x1xi1, #mma1> loc(#loc205)
        %qT_182 = tt.broadcast %mask_qT_179 : tensor<1x16xi1, #blocked1> -> tensor<128x16xi1, #blocked1> loc(#loc195)
        %qT_183 = amdgpu.buffer_load %Q[%qT_ptrs_150], %qT_182 : tensor<128x16xf16, #blocked1> loc(#loc195)
        %dk_184 = ttg.local_load %qT_139 : !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> -> tensor<128x16xf16, #linear1> loc(#loc212)
        %dk_185 = tt.trans %dk_184 {order = array<i32: 1, 0>} : tensor<128x16xf16, #linear1> -> tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc213)
        %qT_186 = ttg.local_load %qT_139 : !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> loc(#loc195)
        %m_187 = arith.addi %offs_m_168, %m_85 : tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc194)
        %m_188 = tt.addptr %m_87, %m_187 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>>, tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc206)
        %m_189 = tt.load %m_188, %mask_m_176, %cst_2 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc206)
        %qkT_190 = tt.dot %k_32, %qT_186, %cst_1 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> * tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> -> tensor<256x16xf32, #mma2> loc(#loc214)
        %pT_191 = arith.mulf %qkT_190, %pT : tensor<256x16xf32, #mma2> loc(#loc192)
        %pT_192 = tt.expand_dims %m_140 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x16xf32, #mma2> loc(#loc215)
        %pT_193 = tt.broadcast %pT_192 : tensor<1x16xf32, #mma2> -> tensor<256x16xf32, #mma2> loc(#loc216)
        %pT_194 = arith.subf %pT_191, %pT_193 : tensor<256x16xf32, #mma2> loc(#loc216)
        %pT_195 = math.exp %pT_194 : tensor<256x16xf32, #mma2> loc(#loc217)
        %do_196 = tt.broadcast %mask_do_180 : tensor<16x1xi1, #blocked> -> tensor<16x128xi1, #blocked> loc(#loc196)
        %do_197 = tt.broadcast %mask_do_181 : tensor<16x1xi1, #mma1> -> tensor<16x128xi1, #mma1> loc(#loc196)
        %do_198 = amdgpu.buffer_load %DO[%do_ptrs_166], %do_196 : tensor<16x128xf16, #blocked> loc(#loc196)
        %dpT_199 = ttg.local_load %do_141 : !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> -> tensor<16x128xf16, #linear2> loc(#loc218)
        %dpT_200 = tt.trans %dpT_199 {order = array<i32: 1, 0>} : tensor<16x128xf16, #linear2> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> loc(#loc219)
        %do_201 = ttg.local_load %do_141 : !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> -> tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc196)
        %dv_202 = arith.truncf %pT_195 : tensor<256x16xf32, #mma2> to tensor<256x16xf16, #mma2> loc(#loc220)
        %dv_203 = ttg.convert_layout %dv_202 : tensor<256x16xf16, #mma2> -> tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc220)
        %dv_204 = tt.dot %dv_203, %do_201, %arg45 : tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x128xf32, #mma> loc(#loc221)
        %Di_205 = tt.addptr %Di, %m_187 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>>, tensor<16xi32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc207)
        %Di_206 = tt.load %Di_205, %mask_m_176 : tensor<16x!tt.ptr<f32>, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc207)
        %dpT_207 = tt.dot %v_42, %dpT_200, %cst_1 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> * tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> -> tensor<256x16xf32, #mma2> loc(#loc218)
        %delta_i_208 = tt.expand_dims %Di_142 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x16xf32, #mma2> loc(#loc222)
        %dsT_209 = tt.broadcast %delta_i_208 : tensor<1x16xf32, #mma2> -> tensor<256x16xf32, #mma2> loc(#loc223)
        %dsT_210 = arith.subf %dpT_207, %dsT_209 : tensor<256x16xf32, #mma2> loc(#loc223)
        %dsT_211 = arith.mulf %pT_195, %dsT_210 : tensor<256x16xf32, #mma2> loc(#loc224)
        %dk_212 = arith.truncf %dsT_211 : tensor<256x16xf32, #mma2> to tensor<256x16xf16, #mma2> loc(#loc225)
        %dq_partial_213 = tt.trans %dk_212 {order = array<i32: 1, 0>} : tensor<256x16xf16, #mma2> -> tensor<16x256xf16, #linear3> loc(#loc226)
        %dk_214 = ttg.convert_layout %dk_212 : tensor<256x16xf16, #mma2> -> tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc225)
        %dk_215 = tt.dot %dk_214, %dk_185, %arg44 : tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x128xf32, #mma> loc(#loc212)
        %dq_partial_216 = ttg.local_alloc %dq_partial_213 : (tensor<16x256xf16, #linear3>) -> !ttg.memdesc<16x256xf16, #shared2, #smem> loc(#loc227)
        %dq_partial_217 = ttg.local_load %dq_partial_216 : !ttg.memdesc<16x256xf16, #shared2, #smem> -> tensor<16x256xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 8}>> loc(#loc227)
        %dq_partial_218 = tt.dot %dq_partial_217, %k_31, %cst_0 : tensor<16x256xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 8}>> * tensor<256x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 8}>> -> tensor<16x128xf32, #mma1> loc(#loc227)
        %20 = arith.mulf %dq_partial_218, %1 : tensor<16x128xf32, #mma1> loc(#loc141)
        %21 = arith.truncf %20 : tensor<16x128xf32, #mma1> to tensor<16x128xf16, #mma1> loc(#loc182)
        %22 = amdgpu.buffer_atomic_rmw fadd, relaxed, gpu, %21, %DQ[%dq_ptrs_162], %do_197 : tensor<16x128xf16, #mma1> loc(#loc182)
        %curr_m_219 = arith.addi %curr_m_137, %c1_i32 : i32 loc(#loc230)
        %curr_m_220 = arith.cmpi slt, %curr_m_219, %c1_i32 : i32 loc(#loc230)
        %curr_m_221 = arith.select %curr_m_220, %curr_m_219, %c0_i32 : i32 loc(#loc230)
        %qT_222 = ttg.memdesc_index %qT[%curr_m_221] : !ttg.memdesc<1x128x16xf16, #shared1, #smem, mutable> -> !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> loc(#loc195)
        ttg.local_store %qT_183, %qT_222 : tensor<128x16xf16, #blocked1> -> !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> loc(#loc195)
        %do_223 = ttg.memdesc_index %do[%curr_m_221] : !ttg.memdesc<1x16x128xf16, #shared, #smem, mutable> -> !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> loc(#loc196)
        ttg.local_store %do_198, %do_223 : tensor<16x128xf16, #blocked> -> !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> loc(#loc196)
        scf.yield %dk_215, %dv_204, %curr_m_221, %curr_m_146, %qT_222, %m_189, %do_223, %Di_206 : tensor<256x128xf32, #mma>, tensor<256x128xf32, #mma>, i32, i32, !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16>, tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>>, !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128>, tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> loc(#loc230)
      } loc(#loc230)
      %dq_ptrs = arith.muli %curr_m_97#3, %stride_dqm : i32 loc(#loc210)
      %dq_ptrs_98 = tt.splat %dq_ptrs : i32 -> tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_99 = tt.expand_dims %offs_m_46 {axis = 1 : i32} : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<16x1xi32, #mma1> loc(#loc211)
      %dq_ptrs_100 = tt.splat %stride_dqm : i32 -> tensor<16x1xi32, #mma1> loc(#loc211)
      %dq_ptrs_101 = arith.muli %dq_ptrs_99, %dq_ptrs_100 : tensor<16x1xi32, #mma1> loc(#loc211)
      %dq_ptrs_102 = tt.broadcast %dq_ptrs_101 : tensor<16x1xi32, #mma1> -> tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_103 = tt.expand_dims %adj_k_16 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #mma1}>> -> tensor<1x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_104 = tt.broadcast %dq_ptrs_103 : tensor<1x128xi32, #mma1> -> tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_105 = arith.addi %dq_ptrs_102, %dq_ptrs_104 : tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_106 = arith.addi %dq_ptrs_105, %dq_ptrs_98 : tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_107 = tt.splat %adj_dq : i32 -> tensor<16x128xi32, #mma1> loc(#loc211)
      %dq_ptrs_108 = arith.addi %dq_ptrs_107, %dq_ptrs_106 : tensor<16x128xi32, #mma1> loc(#loc211)
      %offs_m_109 = tt.splat %curr_m_97#3 : i32 -> tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc203)
      %offs_m_110 = arith.addi %offs_m_109, %offs_m_46 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc203)
      %mask_m_111 = arith.cmpi slt, %offs_m_110, %mask_m_50 : tensor<16xi32, #ttg.slice<{dim = 1, parent = #mma1}>> loc(#loc191)
      %mask_do_112 = tt.expand_dims %mask_m_111 {axis = 1 : i32} : tensor<16xi1, #ttg.slice<{dim = 1, parent = #mma1}>> -> tensor<16x1xi1, #mma1> loc(#loc205)
      %dk_113 = ttg.local_load %curr_m_97#4 : !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> -> tensor<128x16xf16, #linear1> loc(#loc212)
      %dk_114 = tt.trans %dk_113 {order = array<i32: 1, 0>} : tensor<128x16xf16, #linear1> -> tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc213)
      %qT_115 = ttg.local_load %curr_m_97#4 : !ttg.memdesc<128x16xf16, #shared1, #smem, mutable, 1x128x16> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> loc(#loc195)
      %qkT = tt.dot %k_32, %qT_115, %cst_1 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> * tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> -> tensor<256x16xf32, #mma2> loc(#loc214)
      %pT_116 = arith.mulf %qkT, %pT : tensor<256x16xf32, #mma2> loc(#loc192)
      %pT_117 = tt.expand_dims %curr_m_97#5 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x16xf32, #mma2> loc(#loc215)
      %pT_118 = tt.broadcast %pT_117 : tensor<1x16xf32, #mma2> -> tensor<256x16xf32, #mma2> loc(#loc216)
      %pT_119 = arith.subf %pT_116, %pT_118 : tensor<256x16xf32, #mma2> loc(#loc216)
      %pT_120 = math.exp %pT_119 : tensor<256x16xf32, #mma2> loc(#loc217)
      %do_121 = tt.broadcast %mask_do_112 : tensor<16x1xi1, #mma1> -> tensor<16x128xi1, #mma1> loc(#loc196)
      %dpT = ttg.local_load %curr_m_97#6 : !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> -> tensor<16x128xf16, #linear2> loc(#loc218)
      %dpT_122 = tt.trans %dpT {order = array<i32: 1, 0>} : tensor<16x128xf16, #linear2> -> tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> loc(#loc219)
      %do_123 = ttg.local_load %curr_m_97#6 : !ttg.memdesc<16x128xf16, #shared, #smem, mutable, 1x16x128> -> tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> loc(#loc196)
      %dv_124 = arith.truncf %pT_120 : tensor<256x16xf32, #mma2> to tensor<256x16xf16, #mma2> loc(#loc220)
      %dv_125 = ttg.convert_layout %dv_124 : tensor<256x16xf16, #mma2> -> tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc220)
      %dv_126 = tt.dot %dv_125, %do_123, %curr_m_97#1 : tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x128xf32, #mma> loc(#loc221)
      %dpT_127 = tt.dot %v_42, %dpT_122, %cst_1 : tensor<256x128xf16, #ttg.dot_op<{opIdx = 0, parent = #mma2, kWidth = 8}>> * tensor<128x16xf16, #ttg.dot_op<{opIdx = 1, parent = #mma2, kWidth = 8}>> -> tensor<256x16xf32, #mma2> loc(#loc218)
      %delta_i = tt.expand_dims %curr_m_97#7 {axis = 0 : i32} : tensor<16xf32, #ttg.slice<{dim = 0, parent = #mma2}>> -> tensor<1x16xf32, #mma2> loc(#loc222)
      %dsT = tt.broadcast %delta_i : tensor<1x16xf32, #mma2> -> tensor<256x16xf32, #mma2> loc(#loc223)
      %dsT_128 = arith.subf %dpT_127, %dsT : tensor<256x16xf32, #mma2> loc(#loc223)
      %dsT_129 = arith.mulf %pT_120, %dsT_128 : tensor<256x16xf32, #mma2> loc(#loc224)
      %dk_130 = arith.truncf %dsT_129 : tensor<256x16xf32, #mma2> to tensor<256x16xf16, #mma2> loc(#loc225)
      %dq_partial = tt.trans %dk_130 {order = array<i32: 1, 0>} : tensor<256x16xf16, #mma2> -> tensor<16x256xf16, #linear3> loc(#loc226)
      %dk_131 = ttg.convert_layout %dk_130 : tensor<256x16xf16, #mma2> -> tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> loc(#loc225)
      %dk_132 = tt.dot %dk_131, %dk_114, %curr_m_97#0 : tensor<256x16xf16, #ttg.dot_op<{opIdx = 0, parent = #mma, kWidth = 8}>> * tensor<16x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma, kWidth = 8}>> -> tensor<256x128xf32, #mma> loc(#loc212)
      %dq_partial_133 = ttg.local_alloc %dq_partial : (tensor<16x256xf16, #linear3>) -> !ttg.memdesc<16x256xf16, #shared2, #smem> loc(#loc227)
      %dq_partial_134 = ttg.local_load %dq_partial_133 : !ttg.memdesc<16x256xf16, #shared2, #smem> -> tensor<16x256xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 8}>> loc(#loc227)
      %dq_partial_135 = tt.dot %dq_partial_134, %k_31, %cst_0 : tensor<16x256xf16, #ttg.dot_op<{opIdx = 0, parent = #mma1, kWidth = 8}>> * tensor<256x128xf16, #ttg.dot_op<{opIdx = 1, parent = #mma1, kWidth = 8}>> -> tensor<16x128xf32, #mma1> loc(#loc227)
      %17 = arith.mulf %dq_partial_135, %1 : tensor<16x128xf32, #mma1> loc(#loc141)
      %18 = arith.truncf %17 : tensor<16x128xf32, #mma1> to tensor<16x128xf16, #mma1> loc(#loc182)
      %19 = amdgpu.buffer_atomic_rmw fadd, relaxed, gpu, %18, %DQ[%dq_ptrs_108], %do_121 : tensor<16x128xf16, #mma1> loc(#loc182)
      ttg.local_dealloc %do : !ttg.memdesc<1x16x128xf16, #shared, #smem, mutable> loc(#loc230)
      ttg.local_dealloc %qT : !ttg.memdesc<1x128x16xf16, #shared1, #smem, mutable> loc(#loc230)
      scf.yield %dk_132, %dv_126 : tensor<256x128xf32, #mma>, tensor<256x128xf32, #mma> loc(#loc71)
    } loc(#loc193)
    %adj_dkdv = arith.muli %hkid, %stride_dkh : i32 loc(#loc183)
    %2 = tt.expand_dims %offs_n {axis = 1 : i32} : tensor<256xi32, #ttg.slice<{dim = 1, parent = #linear}>> -> tensor<256x1xi32, #linear> loc(#loc73)
    %3 = arith.muli %start_n, %stride_dkn : i32 loc(#loc73)
    %4 = tt.splat %stride_dkn : i32 -> tensor<256x1xi32, #linear> loc(#loc73)
    %5 = arith.muli %2, %4 : tensor<256x1xi32, #linear> loc(#loc73)
    %6 = arith.addi %adj_dkdv, %3 : i32 loc(#loc73)
    %7 = tt.broadcast %5 : tensor<256x1xi32, #linear> -> tensor<256x128xi32, #linear> loc(#loc73)
    %8 = tt.expand_dims %adj_k_14 {axis = 0 : i32} : tensor<128xi32, #ttg.slice<{dim = 0, parent = #linear}>> -> tensor<1x128xi32, #linear> loc(#loc73)
    %9 = tt.broadcast %8 : tensor<1x128xi32, #linear> -> tensor<256x128xi32, #linear> loc(#loc73)
    %10 = arith.addi %7, %9 : tensor<256x128xi32, #linear> loc(#loc73)
    %11 = tt.splat %6 : i32 -> tensor<256x128xi32, #linear> loc(#loc73)
    %12 = arith.addi %11, %10 : tensor<256x128xi32, #linear> loc(#loc73)
    %13 = arith.truncf %dv#1 : tensor<256x128xf32, #mma> to tensor<256x128xf16, #mma> loc(#loc74)
    %14 = ttg.convert_layout %13 : tensor<256x128xf16, #mma> -> tensor<256x128xf16, #linear> loc(#loc74)
    amdgpu.buffer_store %14, %DV[%12], %k_27 : tensor<256x128xf16, #linear> loc(#loc74)
    %dk = tt.splat %sm_scale : f32 -> tensor<256x128xf32, #mma> loc(#loc184)
    %dk_51 = arith.mulf %dv#0, %dk : tensor<256x128xf32, #mma> loc(#loc184)
    %15 = arith.truncf %dk_51 : tensor<256x128xf32, #mma> to tensor<256x128xf16, #mma> loc(#loc76)
    %16 = ttg.convert_layout %15 : tensor<256x128xf16, #mma> -> tensor<256x128xf16, #linear> loc(#loc76)
    amdgpu.buffer_store %16, %DK[%12], %k_27 : tensor<256x128xf16, #linear> loc(#loc76)
    tt.return loc(#loc77)
  } loc(#loc)
} loc(#loc)
#loc1 = loc(unknown)
#loc2 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1772:24)
#loc3 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1777:26)
#loc4 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1778:18)
#loc5 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1778:41)
#loc6 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1796:20)
#loc7 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1799:36)
#loc8 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1799:23)
#loc9 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1800:21)
#loc10 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1800:32)
#loc11 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1808:17)
#loc12 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1811:17)
#loc13 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1815:17)
#loc14 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1821:20)
#loc15 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1821:16)
#loc16 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1822:20)
#loc17 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1822:16)
#loc18 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1824:61)
#loc19 = loc("/app/OAI-triton/python/triton/language/standard.py":41:22)
#loc20 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1857:38)
#loc21 = loc("/app/OAI-triton/python/triton/language/standard.py":41:28)
#loc22 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":604:36)
#loc23 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1895:12)
#loc24 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":612:50)
#loc25 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":644:23)
#loc26 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":645:8)
#loc27 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":660:26)
#loc28 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":694:31)
#loc29 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":766:25)
#loc30 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1824:41)
#loc31 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1825:41)
#loc32 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1826:43)
#loc33 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1831:43)
#loc34 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1833:49)
#loc35 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":649:14)
#loc36 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":685:24)
#loc37 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":670:21)
#loc38 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":702:21)
#loc39 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":652:46)
#loc40 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":654:37)
#loc41 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":655:53)
#loc42 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":655:34)
#loc43 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":657:53)
#loc44 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":657:34)
#loc45 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":659:26)
#loc46 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":661:25)
#loc47 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":662:25)
#loc48 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":685:20)
#loc49 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":730:21)
#loc50 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":650:22)
#loc51 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":652:31)
#loc52 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":656:53)
#loc53 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":656:34)
#loc54 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":753:53)
#loc55 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":753:62)
#loc56 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":691:28)
#loc57 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":694:44)
#loc58 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":694:42)
#loc59 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":694:25)
#loc60 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":736:28)
#loc61 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":736:37)
#loc62 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":727:35)
#loc63 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":727:56)
#loc64 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":741:21)
#loc65 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":742:26)
#loc66 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":742:20)
#loc67 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":753:32)
#loc68 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":763:32)
#loc69 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":763:51)
#loc70 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":766:12)
#loc71 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1859:8)
#loc72 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1905:17)
#loc73 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1910:18)
#loc74 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1910:28)
#loc75 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1911:10)
#loc76 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1912:28)
#loc77 = loc("/opt/conda/envs/py_3.10/lib/python3.10/site-packages/flash_attn-2.8.1-py3.10.egg/flash_attn/flash_attn_triton_amd/bwd.py":1912:4)
#loc118 = loc("wid"(#loc2))
#loc119 = loc("hkid"(#loc3))
#loc120 = loc("pid"(#loc4))
#loc121 = loc("pid"(#loc5))
#loc122 = loc("start_n"(#loc6))
#loc123 = loc("offs_n"(#loc7))
#loc124 = loc("offs_n"(#loc8))
#loc125 = loc("mask_kv"(#loc9))
#loc126 = loc("mask_kv"(#loc10))
#loc127 = loc("adj_k"(#loc11))
#loc128 = loc("adj_k"(#loc12))
#loc129 = loc("adj_v"(#loc13))
#loc130 = loc("k"(#loc14))
#loc131 = loc("k"(#loc15))
#loc132 = loc("v"(#loc16))
#loc133 = loc("v"(#loc17))
#loc134 = loc("num_steps"(#loc20))
#loc135 = loc("offs_m"(#loc22))
#loc136 = loc("qT_ptrs_start"(#loc24))
#loc137 = loc("start_idx"(#loc25))
#loc138 = loc("start_idx"(#loc26))
#loc139 = loc("mask_m"(#loc27))
#loc140 = loc("pT"(#loc28))
#loc141 = loc(callsite(#loc29 at #loc23))
#loc142 = loc("dk"(#loc30))
#loc143 = loc("adj_q"(#loc31))
#loc144 = loc("adj_dq"(#loc32))
#loc145 = loc("adj_do"(#loc33))
#loc146 = loc("adj_delta"(#loc34))
#loc147 = loc(callsite(#loc35 at #loc23))
#loc148 = loc("m"(#loc36))
#loc149 = loc("qT"(#loc37))
#loc150 = loc("do"(#loc38))
#loc151 = loc("blk_idx"(#loc39))
#loc152 = loc("curr_m"(#loc40))
#loc153 = loc("qT_ptrs"(#loc41))
#loc154 = loc("qT_ptrs"(#loc42))
#loc155 = loc("do_ptrs"(#loc43))
#loc156 = loc("do_ptrs"(#loc44))
#loc157 = loc("offs_m"(#loc45))
#loc158 = loc("mask_qT"(#loc46))
#loc159 = loc("mask_do"(#loc47))
#loc160 = loc("m"(#loc48))
#loc161 = loc("Di"(#loc49))
#loc162 = loc("dk"(#loc50))
#loc163 = loc("blk_idx"(#loc51))
#loc164 = loc("dq_ptrs"(#loc52))
#loc165 = loc("dq_ptrs"(#loc53))
#loc166 = loc("dk"(#loc54))
#loc167 = loc("dk"(#loc55))
#loc168 = loc("qkT"(#loc56))
#loc169 = loc("pT"(#loc57))
#loc170 = loc("pT"(#loc58))
#loc171 = loc("pT"(#loc59))
#loc172 = loc("dpT"(#loc60))
#loc173 = loc("dpT"(#loc61))
#loc174 = loc("dv"(#loc62))
#loc175 = loc("dv"(#loc63))
#loc176 = loc("delta_i"(#loc64))
#loc177 = loc("dsT"(#loc65))
#loc178 = loc("dsT"(#loc66))
#loc179 = loc("dk"(#loc67))
#loc180 = loc("dq_partial"(#loc68))
#loc181 = loc("dq_partial"(#loc69))
#loc182 = loc(callsite(#loc70 at #loc23))
#loc183 = loc("adj_dkdv"(#loc72))
#loc184 = loc("dk"(#loc75))
#loc185 = loc(callsite(#loc19 at #loc134))
#loc186 = loc(callsite(#loc21 at #loc134))
#loc187 = loc(callsite(#loc135 at #loc23))
#loc188 = loc(callsite(#loc136 at #loc23))
#loc189 = loc(callsite(#loc137 at #loc23))
#loc190 = loc(callsite(#loc138 at #loc23))
#loc191 = loc(callsite(#loc139 at #loc23))
#loc192 = loc(callsite(#loc140 at #loc23))
#loc193 = loc("dv"(#loc142))
#loc194 = loc(callsite(#loc148 at #loc23))
#loc195 = loc(callsite(#loc149 at #loc23))
#loc196 = loc(callsite(#loc150 at #loc23))
#loc197 = loc(callsite(#loc151 at #loc23))
#loc198 = loc(callsite(#loc152 at #loc23))
#loc199 = loc(callsite(#loc153 at #loc23))
#loc200 = loc(callsite(#loc154 at #loc23))
#loc201 = loc(callsite(#loc155 at #loc23))
#loc202 = loc(callsite(#loc156 at #loc23))
#loc203 = loc(callsite(#loc157 at #loc23))
#loc204 = loc(callsite(#loc158 at #loc23))
#loc205 = loc(callsite(#loc159 at #loc23))
#loc206 = loc(callsite(#loc160 at #loc23))
#loc207 = loc(callsite(#loc161 at #loc23))
#loc208 = loc("dv"(#loc162))
#loc209 = loc(callsite(#loc163 at #loc23))
#loc210 = loc(callsite(#loc164 at #loc23))
#loc211 = loc(callsite(#loc165 at #loc23))
#loc212 = loc(callsite(#loc166 at #loc23))
#loc213 = loc(callsite(#loc167 at #loc23))
#loc214 = loc(callsite(#loc168 at #loc23))
#loc215 = loc(callsite(#loc169 at #loc23))
#loc216 = loc(callsite(#loc170 at #loc23))
#loc217 = loc(callsite(#loc171 at #loc23))
#loc218 = loc(callsite(#loc172 at #loc23))
#loc219 = loc(callsite(#loc173 at #loc23))
#loc220 = loc(callsite(#loc174 at #loc23))
#loc221 = loc(callsite(#loc175 at #loc23))
#loc222 = loc(callsite(#loc176 at #loc23))
#loc223 = loc(callsite(#loc177 at #loc23))
#loc224 = loc(callsite(#loc178 at #loc23))
#loc225 = loc(callsite(#loc179 at #loc23))
#loc226 = loc(callsite(#loc180 at #loc23))
#loc227 = loc(callsite(#loc181 at #loc23))
#loc228 = loc("offs_m"(#loc208))
#loc229 = loc("curr_m"(#loc228))
#loc230 = loc(callsite(#loc229 at #loc23))
